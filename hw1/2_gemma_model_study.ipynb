{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb8bbc9",
   "metadata": {},
   "source": [
    "# Modern Model Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d746ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:37<00:00, 18.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "gemma4b = AutoModel.from_pretrained(\"google/gemma-3-4b-pt\")\n",
    "\n",
    "# Function to count the total parameters for the model\n",
    "def count_model_parameters(model, is_human: bool):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params\n",
    "\n",
    "show_param_count = lambda layer_name, num_params: f\"Number of {layer_name} Parameters: {num_params}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84cb4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3Model(\n",
      "  (vision_tower): SiglipVisionModel(\n",
      "    (vision_model): SiglipVisionTransformer(\n",
      "      (embeddings): SiglipVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "        (position_embedding): Embedding(4096, 1152)\n",
      "      )\n",
      "      (encoder): SiglipEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-26): 27 x SiglipEncoderLayer(\n",
      "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (self_attn): SiglipAttention(\n",
      "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): SiglipMLP(\n",
      "              (activation_fn): PytorchGELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "  )\n",
      "  (language_model): Gemma3TextModel(\n",
      "    (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-33): 34 x Gemma3DecoderLayer(\n",
      "        (self_attn): Gemma3Attention(\n",
      "          (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Gemma3MLP(\n",
      "          (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "          (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "          (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (rotary_emb): Gemma3RotaryEmbedding()\n",
      "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gemma4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da454a",
   "metadata": {},
   "source": [
    "## Vision Encoder Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676e0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_encoder = gemma4b.vision_tower.vision_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc4e9b",
   "metadata": {},
   "source": [
    "### Embedding Layer Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a04ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patch Embedding Parameters: 678528\n",
      "Number of Positional Embedding Parameters: 4718592\n"
     ]
    }
   ],
   "source": [
    "print(show_param_count(\"Patch Embedding\", count_model_parameters(vision_encoder.embeddings.patch_embedding, is_human=False)))\n",
    "print(show_param_count(\"Positional Embedding\", count_model_parameters(vision_encoder.embeddings.position_embedding, is_human=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b96a7c",
   "metadata": {},
   "source": [
    "### Transformer Layer Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd5904f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pre-Attention LayerNorm Parameters: 2304\n",
      "Number of Self-Multi-head Attention Parameters: 5313024\n",
      "Number of Pre-MLP LayerNorm Parameters: 2304\n",
      "Number of MLP Parameters: 9921872\n",
      "Number of Transformer Block Parameters: 15239504\n"
     ]
    }
   ],
   "source": [
    "print(show_param_count(\"Pre-Attention LayerNorm\", count_model_parameters(vision_encoder.encoder.layers[0].layer_norm1, is_human=False)))\n",
    "print(show_param_count(\"Self-Multi-head Attention\", count_model_parameters(vision_encoder.encoder.layers[0].self_attn, is_human=False)))\n",
    "print(show_param_count(\"Pre-MLP LayerNorm\", count_model_parameters(vision_encoder.encoder.layers[0].layer_norm2, is_human=False)))\n",
    "print(show_param_count(\"MLP\", count_model_parameters(vision_encoder.encoder.layers[0].mlp, is_human=False)))\n",
    "print(show_param_count(\"Transformer Block\", count_model_parameters(vision_encoder.encoder.layers[0], is_human=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29df94",
   "metadata": {},
   "source": [
    "### Total Vision Encoder Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8fac7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positional Embedding Parameters: 416866032\n"
     ]
    }
   ],
   "source": [
    "print(show_param_count(\"Positional Embedding\", count_model_parameters(vision_encoder, is_human=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e239a8",
   "metadata": {},
   "source": [
    "## Language Model Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66bc0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_model = gemma4b.language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d36fc1",
   "metadata": {},
   "source": [
    "### Word Embedding Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patch Embedding Parameters: 671252480\n"
     ]
    }
   ],
   "source": [
    "print(show_param_count(\"Word Embedding\", count_model_parameters(lang_model.embed_tokens, is_human=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e675b2",
   "metadata": {},
   "source": [
    "### Transformer Layer Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660e2173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pre-Attention RMSNorm Parameters: 2560\n",
      "Number of Self-Grouped-Query Attention Parameters: 15729152\n",
      "Number of Post-Attention RMSNorm Parameters: 2560\n",
      "Number of Pre-MLP RMSNorm Parameters: 2560\n",
      "Number of MLP Parameters: 78643200\n",
      "Number of Post-MLP RMSNorm Parameters: 2560\n",
      "Number of Transformer Block Parameters: 94382592\n"
     ]
    }
   ],
   "source": [
    "print(show_param_count(\"Pre-Attention RMSNorm\", count_model_parameters(lang_model.layers[0].input_layernorm, is_human=False)))\n",
    "print(show_param_count(\"Self-Grouped-Query Attention\", count_model_parameters(lang_model.layers[0].self_attn, is_human=False)))\n",
    "print(show_param_count(\"Post-Attention RMSNorm\", count_model_parameters(lang_model.layers[0].post_attention_layernorm, is_human=False)))\n",
    "print(show_param_count(\"Pre-MLP RMSNorm\", count_model_parameters(lang_model.layers[0].pre_feedforward_layernorm, is_human=False)))\n",
    "print(show_param_count(\"MLP\", count_model_parameters(lang_model.layers[0].mlp, is_human=False)))\n",
    "print(show_param_count(\"Post-MLP RMSNorm\", count_model_parameters(lang_model.layers[0].post_feedforward_layernorm, is_human=False)))\n",
    "print(show_param_count(\"Transformer Block\", count_model_parameters(lang_model.layers[0], is_human=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af989af6",
   "metadata": {},
   "source": [
    "### Total Language Model Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c21ad303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Language Model Parameters: 3880263168\n"
     ]
    }
   ],
   "source": [
    "print(show_param_count(\"Language Model\", count_model_parameters(lang_model, is_human=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1db9eb",
   "metadata": {},
   "source": [
    "## Multimodal Layer Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "086f3178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mm_input_projection_weight Layer:  torch.Size([1152, 2560])\n",
      "Number of mm_input_projection_weight Parameter:  2949120\n",
      "Shape of mm_soft_emb_norm.weight Layer:  torch.Size([1152])\n",
      "Number of mm_soft_emb_norm.weight Parameter:  1152\n"
     ]
    }
   ],
   "source": [
    "for name, p in gemma4b.multi_modal_projector.named_parameters():\n",
    "    print(f\"Shape of {name} Layer: \", p.shape)\n",
    "    print(f\"Number of {name} Parameter: \", p.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f1ccd",
   "metadata": {},
   "source": [
    "## Total Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16d5a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Gemma3 4B Parameters: 4300079472\n"
     ]
    }
   ],
   "source": [
    "print(show_param_count(\"Gemma3 4B\", count_model_parameters(gemma4b, is_human=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
