{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c693a2e1",
   "metadata": {},
   "source": [
    "# 1. GPT Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "474eaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Function to count the total parameters for the model\n",
    "def count_model_parameters(model, is_human: bool):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56950813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18714061",
   "metadata": {},
   "source": [
    "## Word Embedding Matrix\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{V\\times P}$ be the input sequence to GPT2. The word embedding function is written as\n",
    "$$\\text{Word Embedding}(x)=Wx, \\ W \\in \\mathbb{R}^{E\\times V}$$\n",
    "\n",
    "The number of parameters for the word embedding layer is $V\\times E = 50257\\cdot 768 = 38,597,376$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a124e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding Parameter Count:  38597376\n"
     ]
    }
   ],
   "source": [
    "# Verify the parameter count\n",
    "print(\"Word Embedding Parameter Count: \", count_model_parameters(model.wte, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7d5aa",
   "metadata": {},
   "source": [
    "## Positional Embedding Matrix\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{E \\times P}$, represent the input sequence after the going through the word embedding function. We can write the positional encoding function as,\n",
    "$$\\text{Word Positional Embedding}(x)=W_p + x, \\ W_p\\in\\mathbb{R}^{E\\times P}$$\n",
    "The number of parameters for the word embedding layer is $E\\times P=768\\cdot 1024=786,432$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72403035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed32e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Embedding Parameter Count:  786432\n"
     ]
    }
   ],
   "source": [
    "# Verify the parameter count\n",
    "print(\"Positional Embedding Parameter Count: \", count_model_parameters(model.wpe, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11524c",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20abe14",
   "metadata": {},
   "source": [
    "Let $x\\in\\mathbb{R}^{E\\times P}$ be the input sequence after going through the word embedding and positional embedding matrix, then the we can write the attention mechanism as\n",
    "$$\\text{Attention}(x) = (W_{V}x + b_V)\\text{Softmax}\\left(\\frac{(W_Qx + b_Q)(W_Kx + b_K)^\\top}{\\sqrt{E}}\\right) \\ W_{K, Q, V} \\in \\mathbb{R}^{(E/H)\\times E}, \\ b_{Q, K, V} \\in \\mathbb{R}^{E/H}$$\n",
    "$$\\text{Multi Head Attention}(x) = W_O \\ \\text{concat}(\\text{head}_1,\\dots, \\text{head}_H) + b_O, \\ W_O \\in \\mathbb{R}^{E\\times E}, b_O \\in \\mathbb{R}^E$$\n",
    "The number of parameter for MHA is $4 \\times E^2 + 4\\times E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9df58e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Multi Head Attention Parameter Count:  2362368\n",
      "Multi Head Attention Parameter Count:  2362368\n"
     ]
    }
   ],
   "source": [
    "# Verify parameter count\n",
    "count_attn_params = lambda E: (4 * E) * (E + 1)\n",
    "print(\"Estimated Multi Head Attention Parameter Count: \", count_attn_params(E=768))\n",
    "print(\"Multi Head Attention Parameter Count: \", count_model_parameters(model.h[0].attn, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df2949",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{E\\times P}$ be the input after going through MHA. We can write the MLP layer as,\n",
    "$$\\text{MLP}(x)=W_{\\text{Out}}\\text{GELU}(W_{\\text{In}}x + b_{\\text{In}}) + b_{\\text{Out}}, \\ W_{\\text{In}} \\in \\mathbb{R}^{4E \\times E}, b_{\\text{In}} \\in \\mathbb{R}^{4E}, W_{\\text{Out}} \\in \\mathbb{R}^{E \\times 4E}, b_{\\text{Out}} \\in \\mathbb{R}^E$$\n",
    "The number parameters for the MLP layer is $8 \\times E^2 + 5\\times E = 4,718,592$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55660dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated MLP Layer Parameter Count:  4722432\n",
      "MLP Layer Parameter Count:  4722432\n"
     ]
    }
   ],
   "source": [
    "# Verify parameter count\n",
    "count_mlp_params = lambda E: 8*E**2 + 5*E\n",
    "print(\"Estimated MLP Layer Parameter Count: \", count_mlp_params(E=768))\n",
    "print(\"MLP Layer Parameter Count: \", count_model_parameters(model.h[0].mlp, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae71f1",
   "metadata": {},
   "source": [
    "## LayerNorm\n",
    "\n",
    "The LayerNorm module is explicitly written as,\n",
    "$$\\text{LayerNorm}(x)=\\frac{x-\\text{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}}*\\gamma + \\beta, x, \\gamma, \\beta \\in \\mathbb{R}^{E\\times P}$$\n",
    "Since $x$ is the input variable and $\\epsilon$ is a stabilizing parameter, then the learnable parameters are $\\gamma,\\beta$. The number of parameters for a LayerNorm module is $2\\times E = 1,536$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bff192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm Module Parameter Count:  1536\n"
     ]
    }
   ],
   "source": [
    "# Verify parameter count\n",
    "print(\"LayerNorm Module Parameter Count: \", count_model_parameters(model.h[0].ln_1, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0d67e",
   "metadata": {},
   "source": [
    "## Total Parameter Count for GPT2\n",
    "\n",
    "A transformer block consists of MHA, MLP, and 2 LayerNorms (each before MHA/MLP), then the number of parameters for a transformers is equivalent to $(4 \\times E^2 + 4\\times E) + (8\\times E^2 + 5\\times E) + 2\\times (2\\times E)= 7,087,872$. Since there are $L=12$ transformers block, the final parameter count for GPT2 is equivalent to\n",
    "$$V\\times E + E\\times P + L[(4 \\times E^2 + 4\\times E) + (8\\times E^2 + 5\\times E) + 2\\times (2\\times E)] + 2\\times E \\\\\n",
    "=\\ 38,597,376 + 786,432 + 12*7,087,872 + 1,536 = 124,439,808$$\n",
    "\n",
    "Based on the given formula, the parameter size of GPT2 scales quadratically with the $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1369b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7087872"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_mlp_params(E=768) + count_attn_params(E=768) + 4*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce7e839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated GPT2 Parameters 124439808\n",
      "Total GPT2 Parameters:  124439808\n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters in GPT2\n",
    "def count_gpt_params(V, P, E, L, is_human: bool): \n",
    "    est_params = V * E + P * E + L * (count_attn_params(E) + count_mlp_params(E) + 4*E) + 2*E\n",
    "    return f\"{est_params/1e6:.2f}M\" if is_human else est_params\n",
    "\n",
    "print(\"Estimated GPT2 Parameters\", count_gpt_params(50257, 1024, 768, 12, False))\n",
    "print(\"Total GPT2 Parameters: \", count_model_parameters(model, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484a7db",
   "metadata": {},
   "source": [
    "## Estimation of GPT Sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71115c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated GPT2-Medium Params:  254.05M\n",
      "Estimated GPT2-Large Params:  774.03M\n",
      "Estimated GPT2-XL Params:  1557.61M\n"
     ]
    }
   ],
   "source": [
    "# GPT2 Medium\n",
    "# E = 1024, L = 24\n",
    "print(\"Estimated GPT2-Medium Params: \", count_gpt_params(50257, 1024, 1024, 16, True))\n",
    "\n",
    "#GPT2 Large\n",
    "# E = 1280, L = 36\n",
    "print(\"Estimated GPT2-Large Params: \", count_gpt_params(50257, 1024, 1280, 36, True))\n",
    "\n",
    "#GPT2 XL\n",
    "print(\"Estimated GPT2-XL Params: \", count_gpt_params(50257, 1024, 1600, 48, True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8bbc9",
   "metadata": {},
   "source": [
    "# Modern Model Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d746ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "gemma4b = AutoModel.from_pretrained(\"google/gemma-3-4b-pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9c9da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3Model(\n",
      "  (vision_tower): SiglipVisionModel(\n",
      "    (vision_model): SiglipVisionTransformer(\n",
      "      (embeddings): SiglipVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "        (position_embedding): Embedding(4096, 1152)\n",
      "      )\n",
      "      (encoder): SiglipEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-26): 27 x SiglipEncoderLayer(\n",
      "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (self_attn): SiglipAttention(\n",
      "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): SiglipMLP(\n",
      "              (activation_fn): PytorchGELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "  )\n",
      "  (language_model): Gemma3TextModel(\n",
      "    (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-33): 34 x Gemma3DecoderLayer(\n",
      "        (self_attn): Gemma3Attention(\n",
      "          (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Gemma3MLP(\n",
      "          (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "          (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "          (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "    (rotary_emb): Gemma3RotaryEmbedding()\n",
      "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gemma4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e1956a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model_parameters(gemma4b.language_model.norm, is_human=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfe9b925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3RMSNorm((2560,), eps=1e-06)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma4b.language_model.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"Hello, my name is John! Nice to meet you.\"\n",
    "tokenized_sentence = tokenizer(sample_sentence, return_tensors='pt')\n",
    "embedded_sentence = gemma4b.language_model.embed_tokens(tokenized_sentence.input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
