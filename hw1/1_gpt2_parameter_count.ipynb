{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c693a2e1",
   "metadata": {},
   "source": [
    "# GPT Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474eaed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/radia78/Projects/CS839/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Function to count the total parameters for the model\n",
    "def count_model_parameters(model, is_human: bool):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56950813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18714061",
   "metadata": {},
   "source": [
    "## Word Embedding Matrix\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{V\\times P}$ be the input sequence to GPT2. The word embedding function is written as\n",
    "$$\\text{Word Embedding}(x)=Wx, \\ W \\in \\mathbb{R}^{E\\times V}$$\n",
    "\n",
    "The number of parameters for the word embedding layer is $V\\times E = 50257\\cdot 768 = 38,597,376$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a124e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding Parameter Count:  38597376\n"
     ]
    }
   ],
   "source": [
    "# Verify the parameter count\n",
    "print(\"Word Embedding Parameter Count: \", count_model_parameters(model.wte, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7d5aa",
   "metadata": {},
   "source": [
    "## Positional Embedding Matrix\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{E \\times P}$, represent the input sequence after the going through the word embedding function. We can write the positional encoding function as,\n",
    "$$\\text{Word Positional Embedding}(x)=W_p + x, \\ W_p\\in\\mathbb{R}^{E\\times P}$$\n",
    "The number of parameters for the word embedding layer is $E\\times P=768\\cdot 1024=786,432$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed32e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Embedding Parameter Count:  786432\n"
     ]
    }
   ],
   "source": [
    "# Verify the parameter count\n",
    "print(\"Positional Embedding Parameter Count: \", count_model_parameters(model.wpe, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11524c",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20abe14",
   "metadata": {},
   "source": [
    "Let $x\\in\\mathbb{R}^{E\\times P}$ be the input sequence after going through the word embedding and positional embedding matrix, then the we can write the attention mechanism as\n",
    "$$\\text{Attention}(x) = (W_{V}x + b_V)\\text{Softmax}\\left(\\frac{(W_Qx + b_Q)(W_Kx + b_K)^\\top}{\\sqrt{E}}\\right) \\ W_{K, Q, V} \\in \\mathbb{R}^{(E/H)\\times E}, \\ b_{Q, K, V} \\in \\mathbb{R}^{E/H}$$\n",
    "$$\\text{Multi Head Attention}(x) = W_O \\ \\text{concat}(\\text{head}_1,\\dots, \\text{head}_H) + b_O, \\ W_O \\in \\mathbb{R}^{E\\times E}, b_O \\in \\mathbb{R}^E$$\n",
    "The number of parameter for MHA is $4 \\times E^2 + 4\\times E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9df58e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Multi Head Attention Parameter Count:  2362368\n",
      "Multi Head Attention Parameter Count:  2362368\n"
     ]
    }
   ],
   "source": [
    "# Verify parameter count\n",
    "count_attn_params = lambda E: (4 * E) * (E + 1)\n",
    "print(\"Estimated Multi Head Attention Parameter Count: \", count_attn_params(E=768))\n",
    "print(\"Multi Head Attention Parameter Count: \", count_model_parameters(model.h[0].attn, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df2949",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{E\\times P}$ be the input after going through MHA. We can write the MLP layer as,\n",
    "$$\\text{MLP}(x)=W_{\\text{Out}}\\text{GELU}(W_{\\text{In}}x + b_{\\text{In}}) + b_{\\text{Out}}, \\ W_{\\text{In}} \\in \\mathbb{R}^{4E \\times E}, b_{\\text{In}} \\in \\mathbb{R}^{4E}, W_{\\text{Out}} \\in \\mathbb{R}^{E \\times 4E}, b_{\\text{Out}} \\in \\mathbb{R}^E$$\n",
    "The number parameters for the MLP layer is $8 \\times E^2 + 5\\times E = 4,718,592$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55660dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated MLP Layer Parameter Count:  4722432\n",
      "MLP Layer Parameter Count:  4722432\n"
     ]
    }
   ],
   "source": [
    "# Verify parameter count\n",
    "count_mlp_params = lambda E: 8*E**2 + 5*E\n",
    "print(\"Estimated MLP Layer Parameter Count: \", count_mlp_params(E=768))\n",
    "print(\"MLP Layer Parameter Count: \", count_model_parameters(model.h[0].mlp, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae71f1",
   "metadata": {},
   "source": [
    "## LayerNorm\n",
    "\n",
    "The LayerNorm module is explicitly written as,\n",
    "$$\\text{LayerNorm}(x)=\\frac{x-\\text{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}}*\\gamma + \\beta, x, \\gamma, \\beta \\in \\mathbb{R}^{E\\times P}$$\n",
    "Since $x$ is the input variable and $\\epsilon$ is a stabilizing parameter, then the learnable parameters are $\\gamma,\\beta$. The number of parameters for a LayerNorm module is $2\\times E = 1,536$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bff192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm Module Parameter Count:  1536\n"
     ]
    }
   ],
   "source": [
    "# Verify parameter count\n",
    "print(\"LayerNorm Module Parameter Count: \", count_model_parameters(model.h[0].ln_1, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0d67e",
   "metadata": {},
   "source": [
    "## Total Parameter Count for GPT2\n",
    "\n",
    "A transformer block consists of MHA, MLP, and 2 LayerNorms (each before MHA/MLP), then the number of parameters for a transformers is equivalent to $(4 \\times E^2 + 4\\times E) + (8\\times E^2 + 5\\times E) + 2\\times (2\\times E)= 7,087,872$. Since there are $L=12$ transformers block, the final parameter count for GPT2 is equivalent to\n",
    "$$V\\times E + E\\times P + L[(4 \\times E^2 + 4\\times E) + (8\\times E^2 + 5\\times E) + 2\\times (2\\times E)] + 2\\times E \\\\\n",
    "=\\ 38,597,376 + 786,432 + 12*7,087,872 + 1,536 = 124,439,808$$\n",
    "\n",
    "Based on the given formula, the parameter size of GPT2 scales quadratically with the $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce7e839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated GPT2 Parameters 124439808\n",
      "Total GPT2 Parameters:  124439808\n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters in GPT2\n",
    "def count_gpt_params(V, P, E, L, is_human: bool): \n",
    "    est_params = V * E + P * E + L * (count_attn_params(E) + count_mlp_params(E) + 4*E) + 2*E\n",
    "    return f\"{est_params/1e6:.2f}M\" if is_human else est_params\n",
    "\n",
    "print(\"Estimated GPT2 Parameters\", count_gpt_params(50257, 1024, 768, 12, False))\n",
    "print(\"Total GPT2 Parameters: \", count_model_parameters(model, is_human=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484a7db",
   "metadata": {},
   "source": [
    "## Estimation of GPT Sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71115c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated GPT2-Medium Params:  254.05M\n",
      "Estimated GPT2-Large Params:  774.03M\n",
      "Estimated GPT2-XL Params:  1557.61M\n"
     ]
    }
   ],
   "source": [
    "# GPT2 Medium\n",
    "# E = 1024, L = 24\n",
    "print(\"Estimated GPT2-Medium Params: \", count_gpt_params(50257, 1024, 1024, 16, True))\n",
    "\n",
    "#GPT2 Large\n",
    "# E = 1280, L = 36\n",
    "print(\"Estimated GPT2-Large Params: \", count_gpt_params(50257, 1024, 1280, 36, True))\n",
    "\n",
    "#GPT2 XL\n",
    "# E = 1600, L = 48\n",
    "print(\"Estimated GPT2-XL Params: \", count_gpt_params(50257, 1024, 1600, 48, True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
